{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook serves as a development ground for the new alignment protocol. Note that the clustal omega version being used was downloaded on 10/26/2020 from \"http://www.clustal.org/omega/\" and was renamed to \"clustalo\" from the binary file clustalo-1.2.4-Ubuntu-x86_64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary modules\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Align.Applications import ClustalOmegaCommandline\n",
    "from Bio import pairwise2\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import subprocess\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define globals that will be used for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BARCODE_LENGTH = 7\n",
    "ADAPTER_LENGTH_F = 20\n",
    "ADAPTER_LENGTH_R = 19\n",
    "\n",
    "ALLOWED_WELLS = {'A01', 'A02', 'A03', 'A04', 'A05', 'A06', 'A07', 'A08', 'A09', \n",
    "                 'A10', 'A11', 'A12', 'B01', 'B02', 'B03', 'B04', 'B05', 'B06', \n",
    "                 'B07', 'B08', 'B09', 'B10', 'B11', 'B12', 'C01', 'C02', 'C03', \n",
    "                 'C04', 'C05', 'C06', 'C07', 'C08', 'C09', 'C10', 'C11', 'C12', \n",
    "                 'D01', 'D02', 'D03', 'D04', 'D05', 'D06', 'D07', 'D08', 'D09', \n",
    "                 'D10', 'D11', 'D12', 'E01', 'E02', 'E03', 'E04', 'E05', 'E06', \n",
    "                 'E07', 'E08', 'E09', 'E10', 'E11', 'E12', 'F01', 'F02', 'F03', \n",
    "                 'F04', 'F05', 'F06', 'F07', 'F08', 'F09', 'F10', 'F11', 'F12', \n",
    "                 'G01', 'G02', 'G03', 'G04', 'G05', 'G06', 'G07', 'G08', 'G09', \n",
    "                 'G10', 'G11', 'G12', 'H01', 'H02', 'H03', 'H04', 'H05', 'H06', \n",
    "                 'H07', 'H08', 'H09', 'H10', 'H11', 'H12'}\n",
    "\n",
    "BP_TO_IND = {\"A\": 0, \n",
    "            \"T\": 1,\n",
    "            \"C\": 2,\n",
    "            \"G\": 3,\n",
    "            \"N\": 4,\n",
    "            \"-\": 5}\n",
    "IND_TO_BP = {val: key for key, val in BP_TO_IND.items()}\n",
    "BP_ARRAY = np.array([IND_TO_BP[i] for i in range(len(IND_TO_BP))])\n",
    "\n",
    "\n",
    "AA_TO_IND = {'A': 0, 'R': 1, 'N': 2, 'D': 3, 'C': 4, 'E': 5, 'Q': 6, 'G': 7, \n",
    "             'H': 8, 'I': 9, 'L': 10, 'K': 11, 'M': 12, 'F': 13, 'P': 14, \n",
    "             'S': 15, 'T': 16, 'W': 17, 'Y': 18, 'V': 19, '*': 20, '?': 21, '-': 22}\n",
    "IND_TO_AA = {val: key for key, val in AA_TO_IND.items()}\n",
    "AA_ARRAY = np.array([IND_TO_AA[i] for i in range(len(IND_TO_AA))])\n",
    "\n",
    "CODON_TABLE = {'TTT': 'F', 'TTC': 'F', 'TTA': 'L', 'TTG': 'L', 'TCT': 'S', \n",
    "               'TCC': 'S', 'TCA': 'S', 'TCG': 'S', 'TAT': 'Y', 'TAC': 'Y', \n",
    "               'TGT': 'C', 'TGC': 'C', 'TGG': 'W', 'CTT': 'L', 'CTC': 'L', \n",
    "               'CTA': 'L', 'CTG': 'L', 'CCT': 'P', 'CCC': 'P', 'CCA': 'P', \n",
    "               'CCG': 'P', 'CAT': 'H', 'CAC': 'H', 'CAA': 'Q', 'CAG': 'Q', \n",
    "               'CGT': 'R', 'CGC': 'R', 'CGA': 'R', 'CGG': 'R', 'ATT': 'I', \n",
    "               'ATC': 'I', 'ATA': 'I', 'ATG': 'M', 'ACT': 'T', 'ACC': 'T', \n",
    "               'ACA': 'T', 'ACG': 'T', 'AAT': 'N', 'AAC': 'N', 'AAA': 'K', \n",
    "               'AAG': 'K', 'AGT': 'S', 'AGC': 'S', 'AGA': 'R', 'AGG': 'R', \n",
    "               'GTT': 'V', 'GTC': 'V', 'GTA': 'V', 'GTG': 'V', 'GCT': 'A', \n",
    "               'GCC': 'A', 'GCA': 'A', 'GCG': 'A', 'GAT': 'D', 'GAC': 'D', \n",
    "               'GAA': 'E', 'GAG': 'E', 'GGT': 'G', 'GGC': 'G', 'GGA': 'G', \n",
    "               'GGG': 'G', 'TAG': '*', 'TAA': '*', 'TGA': '*'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write an alignment function wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine the BioPython aligment function so that we can quicky change\n",
    "# parameters later\n",
    "def deseq_align(reference, query):\n",
    "    \n",
    "    # Redefine biopython aligment function (this is just for code neatness)\n",
    "    return pairwise2.align.globalxs(reference, query, open = -2, extend = -1,\n",
    "                                    one_alignment_only=True, penalize_end_gaps = False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a class that holds each sequence pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an object that holds BioPython SeqRecords\n",
    "class SeqPair():\n",
    "    \n",
    "    # Record that we don't have forward or reverse information yet. Record that\n",
    "    # we don't have alignment information yet\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Did sequence pass QC?\n",
    "        self._use_f = False\n",
    "        self._use_r = False\n",
    "        \n",
    "        # Did alignments pass QC?\n",
    "        self._use_f_alignment = False\n",
    "        self._use_r_alignment = False\n",
    "            \n",
    "    # Assign forward reads\n",
    "    def assign_f(self, f_record):\n",
    "        \n",
    "        # Build summary stats\n",
    "        self._f_barcode, self._f_len, self._f_average_q = self.calculate_read_stats(f_record)\n",
    "                \n",
    "        # Assign the forward barcode and the adapterless sequence\n",
    "        self._f_adapterless = f_record[(BARCODE_LENGTH + ADAPTER_LENGTH_F):]\n",
    "        \n",
    "        # Note that we have forward information\n",
    "        self._use_f = True\n",
    "    \n",
    "    # Assign a paired reverse read\n",
    "    def assign_r(self, r_record):\n",
    "        \n",
    "        # Build summary stats\n",
    "        self._r_barcode, self._r_len, self._r_average_q = self.calculate_read_stats(r_record)\n",
    "        \n",
    "        # Assign the reverse barcode and adapterless sequence. \n",
    "        # We want to have the reverse complement of this sequence to match\n",
    "        # the reference sequence\n",
    "        self._sliced_r = r_record[(BARCODE_LENGTH + ADAPTER_LENGTH_R):]\n",
    "        self._r_adapterless = self.sliced_r.reverse_complement(id = True, description = True)\n",
    "        \n",
    "        # Note that we have reverse information\n",
    "        self._use_r = True\n",
    "        \n",
    "    # Calculate summary stats for a read\n",
    "    def calculate_read_stats(self, record):\n",
    "        \n",
    "        # Calculate relevant summary stats needed by both forward and reverse methods\n",
    "        barcode = str(record.seq)[:BARCODE_LENGTH]\n",
    "        length = len(record)\n",
    "        average_quality = np.mean(record.letter_annotations[\"phred_quality\"])\n",
    "        \n",
    "        return barcode, length, average_quality\n",
    "    \n",
    "    # Write a function that performs QC on reads\n",
    "    def qc_reads(self, length_filter, average_q_cutoff):\n",
    "        \n",
    "        # Make sure forward reads pass the length and quality cutoff. Only run QC\n",
    "        # if we actually recorded a forward read.\n",
    "        if self.use_f:\n",
    "            if self.f_len < length_filter or self.f_average_q < average_q_cutoff:\n",
    "                self._use_f = False\n",
    "                \n",
    "        # Make sure reverse reads pass the length and quality cutoff. Only run QC\n",
    "        # if we actually recorded a forward read.\n",
    "        if self.use_r:\n",
    "            if self.r_len < length_filter or self._r_average_q < average_q_cutoff:\n",
    "                self._use_r = False\n",
    "    \n",
    "    # Align forward and reverse reads to a reference sequence\n",
    "    def align(self, reference):\n",
    "        \n",
    "        # Make a pairwise alignment. Only align the reads we are using.\n",
    "        if self.is_paired():\n",
    "            self._f_alignment = deseq_align(reference, self.f_adapterless.seq)\n",
    "            self._r_alignment = deseq_align(reference, self.r_adapterless.seq)\n",
    "            \n",
    "        # If we are only using forward read, handle this here\n",
    "        elif self.use_f:\n",
    "            self._f_alignment = deseq_align(reference, self.f_adapterless.seq)\n",
    "            self._r_alignment = None\n",
    "            \n",
    "        # If we are only using reverse read, handle this here\n",
    "        elif self.use_r:\n",
    "            self._f_alignment = None\n",
    "            self._r_alignment = deseq_align(reference, self.r_adapterless.seq)\n",
    "            \n",
    "        else:\n",
    "            raise AssertionError(\"No reads to align in reference.\")\n",
    "        \n",
    "    # Write a function that runs QC on an alignment. We automatically discard an alignment\n",
    "    # with an insertion or deletion. \n",
    "    def qc_alignment(self, forward_check):\n",
    "        \n",
    "        # By default, this is a good alignment. If it fails the qc tests, then it\n",
    "        # will become a bad alignment\n",
    "        good_alignment = True\n",
    "        \n",
    "        # Pull the appropriate alignment \n",
    "        test_alignment = self.f_alignment if forward_check else self.r_alignment\n",
    "        \n",
    "        # If the alignment is None, this is a bad alignment (the original sequence\n",
    "        # failed qc) and we cannot conitnue.\n",
    "        if test_alignment is None:\n",
    "            return False, -1\n",
    "        \n",
    "        # Pull the reference sequence and alignmed sequence\n",
    "        refseq = test_alignment.seqA\n",
    "        aligned_seq = test_alignment.seqB\n",
    "\n",
    "        # If there are any dashes in the reference sequence, we have a bad alignment\n",
    "        if \"-\" in refseq:\n",
    "            good_alignment = False\n",
    "\n",
    "        # Get the stripped down aligned sequences\n",
    "        lstripped = aligned_seq.lstrip(\"-\")\n",
    "        rstripped = aligned_seq.rstrip(\"-\")\n",
    "        \n",
    "        # If this is a forward check, dashes in the middle or to the left of the aligned\n",
    "        # sequence indicate a deletion or insertion\n",
    "        if forward_check:\n",
    "\n",
    "            # Check to see if we have an insertion or deletion\n",
    "            if len(lstripped) < len(aligned_seq) or \"-\" in rstripped:\n",
    "                good_alignment = False\n",
    "\n",
    "            # Get the first instance of a dash in the full sequence. This indicates the\n",
    "            # first character after the alignment ends\n",
    "            first_dash = lstripped.find(\"-\")\n",
    "\n",
    "            return good_alignment, first_dash\n",
    "\n",
    "        # If this is a reverse check, dashes in the middle or to the right of the aligned\n",
    "        # sequence indicate a deletion or insertion\n",
    "        else:\n",
    "\n",
    "            # Check to see if we have an insertion or deletion\n",
    "            if len(rstripped) < len(aligned_seq) or \"-\" in lstripped:\n",
    "                good_alignment = False\n",
    "\n",
    "            # Get the last instance of a dash in the full sequence. This indicates the last\n",
    "            # character index before the aligned sequence begins.\n",
    "            last_dash = rstripped.rfind(\"-\")\n",
    "\n",
    "            return good_alignment, last_dash\n",
    "\n",
    "    # Write a function that runs QC on a pair of alignments. This will set flags for whether\n",
    "    # or not an alignment is usable\n",
    "    def qc_alignments(self):\n",
    "        \n",
    "        # Run QC on the forward and reverse alignments\n",
    "        self._use_f_alignment, self._first_dash = self.qc_alignment(True)\n",
    "        self._use_r_alignment, self._last_dash = self.qc_alignment(False)\n",
    "        \n",
    "    # Build a composite alignment for paired ends\n",
    "    def build_paired_composite_alignment(self):\n",
    "        \n",
    "        # Both forward and reverse reads must pass aligment qc to enable this \n",
    "        assert self.is_paired_post_alignment_qc(), \"Cannot build composite from 1 read.\"\n",
    "        \n",
    "        # Grab the reference sequence, the aligned sequences, \n",
    "        # and the quality scores\n",
    "        refseq = self.f_alignment.seqA\n",
    "        reflength = len(refseq)\n",
    "        forward_seq = self.f_alignment.seqB\n",
    "        reverse_seq = self.r_alignment.seqB\n",
    "        forward_qual = np.array(self.f_adapterless.letter_annotations[\"phred_quality\"])\n",
    "        reverse_qual = np.array(self.r_adapterless.letter_annotations[\"phred_quality\"])\n",
    "\n",
    "        # Get the end of the f read. If it goes all the way to the end of the reference\n",
    "        # sequence, then the first non-f character is the length of the sequence\n",
    "        post_forward_dash_ind = reflength if self.first_dash == -1 else self.first_dash\n",
    "        last_forward_char_ind = post_forward_dash_ind - 1\n",
    "\n",
    "        # Get the beginning of the r read. If it starts from the beginning of the ference\n",
    "        # sequence, then the first non-r character is -1, so we don't actually need to\n",
    "        # make any adjustments\n",
    "        pre_reverse_dash_ind = self.last_dash\n",
    "        first_r_char_ind = pre_reverse_dash_ind + 1\n",
    "\n",
    "        # See if the forward and reverse overlap. If they don't overlap. Then the composite\n",
    "        # is just the forward DNA + dashes + reverse DNA\n",
    "        if post_forward_dash_ind <= pre_reverse_dash_ind:\n",
    "\n",
    "            # Calculate the number of dashes needed\n",
    "            n_dashes = pre_reverse_dash_indr - post_forward_dash_ind + 1\n",
    "\n",
    "            # Build the composite sequence between the two\n",
    "            composite_seq = \"\".join((forward_seq[:post_forward_dash_ind], \n",
    "                                   \"-\" * n_dashes,\n",
    "                                   reverse_seq[first_r_char_ind:]))\n",
    "\n",
    "            # Build the composite quality. The quality scores are not extended for the\n",
    "            # alignment, and so map directly to the pulled sequences.\n",
    "            composite_qual = np.concatenate((forward_qual,\n",
    "                                             np.full(n_dashes, np.inf),\n",
    "                                             reverse_qual))\n",
    "\n",
    "        # Otherwise, take the sequence with the highest quality in the overlapping region\n",
    "        else:\n",
    "\n",
    "            # Pull the forward up to the start of the reverse sequence\n",
    "            only_f_seq = forward_seq[:first_r_char_ind]\n",
    "            only_f_qual = forward_qual[:first_r_char_ind]\n",
    "\n",
    "            # Pull the reverse after the end point of forward. Quality scores only cover \n",
    "            # sequence (not alignment gaps), so we need to calculate where the qualities\n",
    "            # end for the reverse sequence.\n",
    "            only_r_seq = reverse_seq[post_forward_dash_ind:]\n",
    "            reverse_qual_break = len(reverse_qual) - len(only_r_seq)\n",
    "            only_r_qual = reverse_qual[reverse_qual_break:]\n",
    "\n",
    "            # Now compare the middle parts. Take the one with the higher sequence quality. \n",
    "            # The middle characters all fall \n",
    "            middle_f_seq = forward_seq[first_r_char_ind:post_forward_dash_ind]\n",
    "            middle_f_qual = forward_qual[first_r_char_ind:]\n",
    "            middle_r_seq = reverse_seq[first_r_char_ind:post_forward_dash_ind]\n",
    "            middle_r_qual = reverse_qual[:reverse_qual_break]\n",
    "\n",
    "            # The middle sequences should be equal in length (they might differ in sequence\n",
    "            # due to sequencing errors.  The quality scores should have the same length as well\n",
    "            middle_size = len(middle_f_seq)\n",
    "            assert middle_size == len(middle_r_seq)\n",
    "            assert middle_size == len(middle_f_qual)\n",
    "            assert middle_size == len(middle_r_qual)\n",
    "\n",
    "            # Build the composite middle sequence and quality\n",
    "            middle_seq = [None] * middle_size\n",
    "            middle_qual = np.zeros(middle_size, dtype = int)\n",
    "            quality_comparison = np.greater(middle_f_qual, middle_r_qual).astype(int)\n",
    "            for i in range(middle_size):\n",
    "\n",
    "                # If the reverse read has better quality, use that\n",
    "                if quality_comparison[i]:\n",
    "                    middle_seq[i] = middle_r_seq[i]\n",
    "                    middle_qual[i] = middle_r_qual[i]\n",
    "\n",
    "                # If the forward read has better quality, use that\n",
    "                else:\n",
    "                    middle_seq[i] = middle_f_seq[i]\n",
    "                    middle_qual[i] = middle_f_qual[i]\n",
    "\n",
    "            # Build the overall composite sequence and qualities. \n",
    "            composite_seq = \"\".join((only_f_seq, \"\".join(middle_seq), only_r_seq))\n",
    "            composite_qual = np.concatenate((only_f_qual, middle_qual, only_r_qual))\n",
    "            \n",
    "        # Check to be sure lengths are correct\n",
    "        assert reflength == len(composite_seq)\n",
    "        assert reflength == len(composite_qual)\n",
    "            \n",
    "        return composite_seq, composite_qual\n",
    "    \n",
    "    # Build a pairwise composite alignment for non-paired ends\n",
    "    def build_unpaired_composite_alignment(self):\n",
    "        \n",
    "        # First make sure that we are calling this function appropriately\n",
    "        assert not self.is_paired_post_alignment_qc(), \"This function only works for unpaired reads\"\n",
    "        \n",
    "        # Determine if it is forward or reverse reads\n",
    "        if self.use_f_alignment:\n",
    "            \n",
    "            # Get the length of the reference sequence\n",
    "            refseq = self.f_alignment.seqA\n",
    "            composite_length = len(refseq)\n",
    "            \n",
    "            # The composite sequence is just the aligned sequence\n",
    "            composite_seq = self.f_alignment.seqB\n",
    "            \n",
    "            # The qualities continue after the alignment. Add as many zeros as \n",
    "            # there are differences between existing qualities and the end of\n",
    "            # the sequence\n",
    "            forward_qual = self.f_adapterless.letter_annotations[\"phred_quality\"]\n",
    "            composite_qual = np.concatenate((forward_qual, \n",
    "                                             np.full(composite_length - len(forward_qual), np.inf)))\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # Get the length of the reference sequence\n",
    "            refseq = self.r_alignment.seqA\n",
    "            composite_length = len(refseq)\n",
    "            \n",
    "            # The composite sequence is just the aligned sequence\n",
    "            composite_seq = self.r_alignment.seqB\n",
    "            \n",
    "            # The qualities must be before the alignment. Prepend as many zeros\n",
    "            # as there are differences between existing qualities and the end of \n",
    "            # the sequence\n",
    "            reverse_qual = self.r_adapterless.letter_annotations[\"phred_quality\"]\n",
    "            composite_qual = np.concatenate((np.full(composite_length - len(reverse_qual), np.inf),\n",
    "                                             reverse_qual))\n",
    "            \n",
    "        # Assert that everything is the expected length\n",
    "        assert composite_length == len(composite_seq)\n",
    "        assert composite_length == len(composite_qual)\n",
    "            \n",
    "        return composite_seq, composite_qual\n",
    "    \n",
    "    # Write a function that builds a composite sequence regardless of alignment type\n",
    "    def build_composite_alignment(self):\n",
    "        \n",
    "        # Complicated composite if this is paired end\n",
    "        if self.is_paired_post_alignment_qc():\n",
    "            return self.build_paired_composite_alignment()\n",
    "        \n",
    "        # Simple composite if this is not paired end\n",
    "        else:\n",
    "            return self.build_unpaired_composite_alignment()\n",
    "        \n",
    "    # Write a function for extracting information from the alignment\n",
    "    def analyze_alignment(self, inframe_ind, ref_len, n_aas, qual_thresh):\n",
    "        \n",
    "        # Pull the composite alignment for the sequence\n",
    "        composite_sequence, composite_qual = self.build_composite_alignment()\n",
    "\n",
    "        # Create matrices in which to store counts\n",
    "        bp_counts = np.zeros([6, ref_len], dtype = int)\n",
    "        aa_counts = np.zeros([23, n_aas], dtype = int)\n",
    "\n",
    "        # Loop over the composite sequence up to the in-frame part\n",
    "        base_ind = -1 # Initilaize for the case where inframe_ind is 0\n",
    "        for base_ind, (bp, qual) in enumerate(zip(composite_sequence[:inframe_ind],\n",
    "                                                  composite_qual[:inframe_ind])):\n",
    "\n",
    "            # Only record counts if we meet a quality threshold\n",
    "            if qual >= qual_thresh:\n",
    "                bp_counts[BP_TO_IND[bp], base_ind] += 1\n",
    "\n",
    "        # Initialize variables for holding codon information\n",
    "        aa_counter = 0\n",
    "        record_aa = True\n",
    "        codon = [None] * 3\n",
    "        codon_counter = 0\n",
    "        \n",
    "        # Loop over the remaining sequence that is in frame\n",
    "        for inframe_counter, (bp, qual) in enumerate(zip(composite_sequence[inframe_ind:],\n",
    "                                                         composite_qual[inframe_ind:])):\n",
    "\n",
    "            # Update the base ind (this continues from our previous loop)\n",
    "            base_ind += 1\n",
    "\n",
    "            # Only record counts if we meet a quality threshold\n",
    "            if qual >= qual_thresh:\n",
    "                bp_counts[BP_TO_IND[bp], base_ind] += 1\n",
    "                codon[codon_counter] = bp\n",
    "\n",
    "            # If we don't meet a quality threshold, then throw a flag to\n",
    "            # not record the aa in this codon\n",
    "            else:\n",
    "                record_aa = False\n",
    "\n",
    "            # Increment the codon counter\n",
    "            codon_counter += 1\n",
    "            \n",
    "            # If this is the third character in a codon reset the codon counter\n",
    "            # and other codon-related variables\n",
    "            if (inframe_counter + 1) %3 == 0:\n",
    "\n",
    "                # If all members of the codon passed quality control record\n",
    "                if record_aa:\n",
    "                    \n",
    "                    # Join the characters\n",
    "                    joined_codon = \"\".join(codon)\n",
    "                    \n",
    "                    # If this is in a gap, record gap\n",
    "                    if \"-\" in joined_codon:\n",
    "                        aa = \"-\"\n",
    "                    \n",
    "                    # If it isn't in the codon table, record question mark\n",
    "                    elif joined_codon not in CODON_TABLE:\n",
    "                        aa = \"?\"\n",
    "                    \n",
    "                    else:\n",
    "                        aa = CODON_TABLE[joined_codon]\n",
    "                    \n",
    "                    # Add to counts\n",
    "                    aa_counts[AA_TO_IND[aa], aa_counter] += 1\n",
    "\n",
    "                # Reset all codon related variables and increment the aa counter\n",
    "                aa_counter += 1\n",
    "                record_aa = True\n",
    "                codon = [None] * 3\n",
    "                codon_counter = 0\n",
    "            \n",
    "        # Run a check on the count. A sum across the 0th axis should\n",
    "        # return all ones and zeros, as we should never count two bases or two\n",
    "        # amino acids in one position\n",
    "        bp_test = np.sum(bp_counts, axis = 0)\n",
    "        aa_test = np.sum(aa_counts, axis = 0)\n",
    "        assert np.all(np.logical_or(bp_test == 1, bp_test == 0)), \"Double counting bases\"\n",
    "        assert np.all(np.logical_or(aa_test == 1, aa_test == 0)), \"Double counting amino acids\"\n",
    "            \n",
    "        # Return the filled out count matrices\n",
    "        return bp_counts, aa_counts\n",
    "    \n",
    "    # Write a function that returns read lengths\n",
    "    def read_lengths(self):\n",
    "        if self.is_paired():\n",
    "            return [self.f_len, self.r_len]\n",
    "        elif self.use_f:\n",
    "            return [self.f_len, np.nan]\n",
    "        elif self.use_r:\n",
    "            return [np.nan, self.r_len]\n",
    "        else:\n",
    "            raise AssertionError(\"No reads for which to return lengths.\")\n",
    "    \n",
    "    # Check to see if we are using both sequences\n",
    "    def is_paired(self):\n",
    "        if self.use_r and self.use_f:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    # Check to see if we have no sequences aligned\n",
    "    def is_dud(self):\n",
    "        if not (self.use_r or self.use_f):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    # Check to see if both alignments pass QC\n",
    "    def is_paired_post_alignment_qc(self):\n",
    "        if self.use_f_alignment and self.use_r_alignment:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    # Check to see if we have no alignments that pass\n",
    "    def is_dud_post_alignment_qc(self):\n",
    "        if not(self.use_f_alignment or self.use_r_alignment):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    # Make all the properties\n",
    "    @property\n",
    "    def use_f(self):\n",
    "        return self._use_f\n",
    "    \n",
    "    @property\n",
    "    def use_r(self):\n",
    "        return self._use_r\n",
    "    \n",
    "    @property\n",
    "    def use_f_alignment(self):\n",
    "        return self._use_f_alignment\n",
    "    \n",
    "    @property\n",
    "    def use_r_alignment(self):\n",
    "        return self._use_r_alignment\n",
    "        \n",
    "    @property\n",
    "    def f_barcode(self):\n",
    "        return self._f_barcode\n",
    "    \n",
    "    @property\n",
    "    def f_len(self):\n",
    "        return self._f_len\n",
    "    \n",
    "    @property\n",
    "    def f_average_q(self):\n",
    "        return self._f_average_q\n",
    "    \n",
    "    @property\n",
    "    def f_adapterless(self):\n",
    "        return self._f_adapterless\n",
    "    \n",
    "    @property\n",
    "    def r_barcode(self):\n",
    "        return self._r_barcode\n",
    "    \n",
    "    @property\n",
    "    def r_len(self):\n",
    "        return self._r_len\n",
    "    \n",
    "    @property\n",
    "    def r_average_q(self):\n",
    "        return self._r_average_q\n",
    "    \n",
    "    @property\n",
    "    def sliced_r(self):\n",
    "        return self._sliced_r\n",
    "    \n",
    "    @property\n",
    "    def r_adapterless(self):\n",
    "        return self._r_adapterless\n",
    "    \n",
    "    @property\n",
    "    def f_alignment(self):\n",
    "        return self._f_alignment\n",
    "    \n",
    "    @property\n",
    "    def r_alignment(self):\n",
    "        return self._r_alignment\n",
    "    \n",
    "    @property\n",
    "    def first_dash(self):\n",
    "        return self._first_dash\n",
    "    \n",
    "    @property\n",
    "    def last_dash(self):\n",
    "        return self._last_dash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a class that holds wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Well():\n",
    "    \n",
    "    # Initialization assigns attributes, reference sequences, and sequence pairs\n",
    "    def __init__(self, seqpairs, refseq_df_info, save_dir):\n",
    "        \n",
    "        # Assign the sequence pairs as an attribute, unpack the refseq info, and store\n",
    "        # the expected variable basepair positions as attirbutes\n",
    "        self._all_seqpairs = seqpairs\n",
    "        self._expected_variable_bp_positions = refseq_df_info[\"ExpectedVariablePositions\"]\n",
    "        self._index_plate = refseq_df_info[\"IndexPlate\"]\n",
    "        self._plate_nickname = refseq_df_info[\"PlateNickname\"]\n",
    "        self._well = refseq_df_info[\"Well\"]\n",
    "        self._reference_sequence = refseq_df_info[\"ReferenceSequence\"]\n",
    "        self._ref_len = len(self.reference_sequence)\n",
    "        self._in_frame_ind = refseq_df_info[\"InFrameBase\"] - 1 #Input is 1-indexed, so subtract 1\n",
    "        self._bp_ind_start = refseq_df_info[\"BpIndStart\"]\n",
    "        self._aa_ind_start = refseq_df_info[\"AAIndStart\"]\n",
    "        \n",
    "        # Generate save locations for alignment files\n",
    "        self._fasta_loc = os.path.join(save_dir, \"ParsedFilteredFastqs\")\n",
    "        self._alignment_loc = os.path.join(save_dir, \"Alignments\", \n",
    "                                       f\"{self.index_plate}-{self.well}.txt\")\n",
    "        \n",
    "        # Get the number of aas in the reference sequence\n",
    "        self._n_aas = (self.ref_len - self.in_frame_ind) // 3\n",
    "        \n",
    "        # Calculate the expected count frequencies for both basepairs and\n",
    "        # amino acids assuming no sequencing errors and no changes to reference sequence\n",
    "        self.calculate_expected_arrays()\n",
    "        \n",
    "        # Calculate the variable amino acid positions\n",
    "        self.calculate_expected_variable_aa_positions()\n",
    "        \n",
    "        # Create placeholders for a number of attributes. This is done to allow \n",
    "        # failing gracefully out of the analyze count functions\n",
    "        # if we don't have any reads in a well\n",
    "        self._unit_bp_freqs_no_gaps = None\n",
    "        self._bp_position_counts = None\n",
    "        self._all_variable_bp_positions = None\n",
    "        self._variable_bp_type = None\n",
    "        \n",
    "        self._unit_aa_freqs_no_gaps = None\n",
    "        self._aa_position_counts = None\n",
    "        self._all_variable_aa_positions = None\n",
    "        self._variable_aa_type = None\n",
    "        \n",
    "        self._all_bp_counts = None\n",
    "        self._all_aa_counts = None\n",
    "        \n",
    "    # Write a function to calculate the expected reference amino acid and base sequences\n",
    "    def calculate_expected_arrays(self):\n",
    "    \n",
    "        # Create arrays for storing expected results. \n",
    "        self._expected_bps = np.zeros([6, self.ref_len], dtype = int)\n",
    "        self._expected_aas = np.zeros([23, self.n_aas], dtype = int)\n",
    "                \n",
    "        # Loop over the reference sequence and record expected basepairs\n",
    "        for bp_ind, bp in enumerate(self.reference_sequence):\n",
    "            self._expected_bps[BP_TO_IND[bp], bp_ind] += 1\n",
    "\n",
    "        # Caculate last readable bp for translation\n",
    "        last_readable_bp = self.in_frame_ind + self.n_aas * 3\n",
    "        \n",
    "        # Loop over the codons in the reference sequence and record\n",
    "        aa_counter = 0\n",
    "        for chunker in range(self.in_frame_ind, last_readable_bp, 3):\n",
    "\n",
    "            # Identify the codon and translate\n",
    "            codon = self.reference_sequence[chunker: chunker + 3]\n",
    "            expected_aa = \"?\" if codon not in CODON_TABLE else CODON_TABLE[codon]\n",
    "\n",
    "            # Record and increment counter\n",
    "            self._expected_aas[AA_TO_IND[expected_aa], aa_counter] += 1\n",
    "            aa_counter += 1\n",
    "            \n",
    "        # Make sure we are not double counting and that we are counting everything\n",
    "        bp_test = np.sum(self.expected_bps, axis = 0)\n",
    "        aa_test = np.sum(self.expected_aas, axis = 0)\n",
    "        assert np.all(bp_test == 1), \"Expected bp calculation is wrong\"\n",
    "        assert np.all(aa_test == 1), \"Expected aa calculation is wrong\"\n",
    "        \n",
    "        # Calculate and store the amino acid reference sequence\n",
    "        aa_ref_inds = np.argwhere(np.transpose(self.expected_aas == 1))[:, 1]\n",
    "        self._reference_sequence_aa = \"\".join(AA_ARRAY[aa_ref_inds].tolist())\n",
    "        \n",
    "    # Write a function for calculating the expected variable amino acid positions\n",
    "    def calculate_expected_variable_aa_positions(self):\n",
    "\n",
    "        # Get the number of expected variable basepair positions\n",
    "        n_bp_positions = len(self.expected_variable_bp_positions)\n",
    "        \n",
    "        # If there are none, we have an empty array\n",
    "        if n_bp_positions == 0:\n",
    "            self._expected_variable_aa_positions = np.array([], dtype = int)\n",
    "            \n",
    "        # Otherwise, calculate the positions\n",
    "        else:\n",
    "\n",
    "            # Assert that the positions are sorted, unique, and divisible by 3\n",
    "            assert sorted(self.expected_variable_bp_positions) == \\\n",
    "                self.expected_variable_bp_positions.tolist(), \"Error in basepair sorting\"\n",
    "            assert len(set(self.expected_variable_bp_positions)) == n_bp_positions, \"Duplicate basepairs found\"\n",
    "            assert n_bp_positions % 3 == 0, \"Bp positions not divisible by 3\"\n",
    "\n",
    "            # Loop over the variable bp positions in chunks of 3. \n",
    "            self._expected_variable_aa_positions = np.full(int(n_bp_positions / 3), np.nan,\n",
    "                                                          dtype = int)\n",
    "            position_counter = 0\n",
    "            for chunker in range(0, n_bp_positions, 3):\n",
    "\n",
    "                # Grab the codon\n",
    "                codon = self.expected_variable_bp_positions[chunker: chunker+3]\n",
    "\n",
    "                # Assert that the codon positions are 1 apart\n",
    "                assert (codon[1] - codon[0]) == 1, \"Codon positions not in order\"\n",
    "                assert (codon[2] - codon[0]) == 2, \"Codon positions not in order\"\n",
    "\n",
    "                # Calculate the amino acid position \n",
    "                self._expected_variable_aa_positions[position_counter] = int((codon[0] - self.in_frame_ind) / 3)\n",
    "                position_counter += 1\n",
    "                    \n",
    "    # Write a function that makes pairwise and runs qc on pairwise alignments and then identifies usable\n",
    "    # and paired alignments\n",
    "    def align(self):\n",
    "        \n",
    "        # Run alignment on all seqpairs\n",
    "        for seqpair in self.all_seqpairs:\n",
    "            seqpair.align(self.reference_sequence)\n",
    "            seqpair.qc_alignments()\n",
    "        \n",
    "        # Identify seqpairs that have at least one read passing alignment QC\n",
    "        self._non_dud_alignments = tuple(filter(lambda x: not x.is_dud_post_alignment_qc(), self.all_seqpairs))\n",
    "                \n",
    "    # Write a function that analyzes alignments to generate count matrices\n",
    "    def analyze_alignments(self, qual_thresh, variable_count):\n",
    "\n",
    "        # Get the number of duds. If there we have less alignments that our \n",
    "        # variable threhold, return False\n",
    "        n_non_duds = len(self.non_dud_alignments)\n",
    "        if n_non_duds < variable_count:\n",
    "            self._usable_reads = False\n",
    "            return False\n",
    "        \n",
    "        # Create matrices in which to store counts\n",
    "        self._all_bp_counts = np.zeros([n_non_duds, 6, self.ref_len], dtype = int)\n",
    "        self._all_aa_counts = np.zeros([n_non_duds, 23, self.n_aas], dtype = int)\n",
    "        \n",
    "        # Loop over all non-dud seqpairs and record counts for each aa and sequence\n",
    "        for pair_ind, seqpair in enumerate(self.non_dud_alignments):\n",
    "            (self._all_bp_counts[pair_ind],\n",
    "             self._all_aa_counts[pair_ind]) = seqpair.analyze_alignment(self.in_frame_ind, self.ref_len,\n",
    "                                                                        self.n_aas, qual_thresh) \n",
    "            \n",
    "        # Return true to signifify that we identified at least one non-dud.\n",
    "        self._usable_reads = True\n",
    "        return True\n",
    "                \n",
    "    # Write a function that calculates counts and frequencies by unit (e.g. amino acid or \n",
    "    # base pair) and position in the sequence. \n",
    "    @staticmethod\n",
    "    def build_unit_counts_generic(count_array):\n",
    "        \n",
    "        # Get the counts for each unit (e.g. an amino acid or base pair) at each\n",
    "        # position. For both the aa and bp count matrices, the last row is the gap character.\n",
    "        # The gap character is ignored when generating counts\n",
    "        by_unit_counts = count_array[:, :-1].sum(axis=0)\n",
    "    \n",
    "        # Now get the total counts at each position\n",
    "        by_position_counts = by_unit_counts.sum(axis=0)\n",
    "\n",
    "        # Convert counts for each unit at each position to frequency for\n",
    "        # each unit at each position. Return 0 if the by_position counts\n",
    "        # are also 0 (avoid divide by 0 error)\n",
    "        by_unit_frequency = np.divide(by_unit_counts, by_position_counts,\n",
    "                                     out = np.zeros_like(by_unit_counts, dtype = float),\n",
    "                                     where = by_position_counts != 0)\n",
    "        \n",
    "        # If not keeping gaps, return the by position counts as well as the\n",
    "        # unit counts and frequencies. Otherwise, just return the unit counts\n",
    "        # and frequencies\n",
    "        return by_unit_counts, by_unit_frequency, by_position_counts\n",
    "        \n",
    "    def build_unit_count_matrices(self):\n",
    "        \n",
    "        # Run the generic count calculator for aas and bps, ignoring gaps\n",
    "        (self._unit_bp_counts_no_gaps, \n",
    "         self._unit_bp_freqs_no_gaps,\n",
    "         self._bp_position_counts) = Well.build_unit_counts_generic(self.all_bp_counts)\n",
    "        (self._unit_aa_counts_no_gaps,\n",
    "         self._unit_aa_freqs_no_gaps,\n",
    "         self._aa_position_counts) = Well.build_unit_counts_generic(self.all_aa_counts)\n",
    "    \n",
    "    # Now write a generic function for identifying variable positions\n",
    "    @staticmethod\n",
    "    def identify_variable_positions_generic(by_unit_frequency, expected_array, \n",
    "                                            variable_thresh, expected_variable_positions):\n",
    "        \n",
    "        # Compare the unit frequency to the expected array.\n",
    "        # The furthest difference is 2 (e.g. if there are no reads matching to the\n",
    "        # expected sequence), so take the absolute value is taken and the full\n",
    "        # array divided by 2 to scale to a \"percent different\"\n",
    "        difference_from_expectation_absolute = np.abs(by_unit_frequency - expected_array)\n",
    "        average_difference_from_expectation = np.sum(difference_from_expectation_absolute, axis = 0)/2\n",
    "        \n",
    "        # Get the length of the unit frequency first axis\n",
    "        n_units = by_unit_frequency.shape[0]\n",
    "\n",
    "        # Compare the unit frequency to the expected array.\n",
    "        # The furthest difference is 2 (e.g. if there are no reads matching to the\n",
    "        # expected sequence), so take the absolute value is taken and the full\n",
    "        # array divided by 2 to scale to a \"percent different\"\n",
    "        difference_from_expectation_absolute = np.abs(by_unit_frequency - expected_array[:n_units])\n",
    "        average_difference_from_expectation = np.sum(difference_from_expectation_absolute, axis = 0)/2\n",
    "\n",
    "        # Find positions that have differences greater than the threshold\n",
    "        identified_variable_positions = np.argwhere(average_difference_from_expectation > \n",
    "                                                    variable_thresh).flatten()\n",
    "        identified_variable_positions.sort()\n",
    "        \n",
    "        # Get the unique set of variable positions\n",
    "        expected_set = set(expected_variable_positions)\n",
    "        all_found = np.unique(np.concatenate((expected_variable_positions, \n",
    "                                              identified_variable_positions)))\n",
    "        all_found.sort()\n",
    "        \n",
    "        # Determine if the variation is expected or not. Return this along with all_found\n",
    "        expected_variation = np.array([\"\" if var in expected_set else \"Unexpected Variation\"\n",
    "                                       for var in all_found])\n",
    "        \n",
    "        return all_found, expected_variation\n",
    "        \n",
    "    # Write a function for identifying variable positions in both the amino acid\n",
    "    # and basepair counts\n",
    "    def identify_variable_positions(self, variable_thresh):\n",
    "        \n",
    "        # Find the variable basepair and amino acid positions. Note that gaps are not used \n",
    "        # when finding variable positions\n",
    "        (self._all_variable_bp_positions, \n",
    "         self._variable_bp_type) = Well.identify_variable_positions_generic(self.unit_bp_freqs_no_gaps,\n",
    "                                                                            self.expected_bps[:-1],\n",
    "                                                                            variable_thresh,\n",
    "                                                                           self.expected_variable_bp_positions)\n",
    "        (self._all_variable_aa_positions,\n",
    "         self._variable_aa_type) = Well.identify_variable_positions_generic(self.unit_aa_freqs_no_gaps,\n",
    "                                                                            self.expected_aas[:-1],\n",
    "                                                                            variable_thresh,\n",
    "                                                                           self.expected_variable_aa_positions)\n",
    "    \n",
    "    # Write a function that analyzes and reports unpaired counts\n",
    "    def analyze_unpaired_counts_generic(self, unit_freq_array, total_count_array, \n",
    "                                        all_variable_positions, expectation_array,\n",
    "                                        unit_array, unit_type, variable_thresh,\n",
    "                                        pos_offset):\n",
    "        \n",
    "        # Define output columns\n",
    "        unit_pos = f\"{unit_type}Position\" # Create a name for the unit position\n",
    "        columns = (\"IndexPlate\", \"Plate\", \"Well\",  unit_pos, unit_type,\n",
    "                   \"AlignmentFrequency\", \"WellSeqDepth\", \"Flag\")\n",
    "        \n",
    "        # If there are no reads, return that this is a dead well\n",
    "        if not self.usable_reads:\n",
    "            output_df = pd.DataFrame([[self.index_plate, self.plate_nickname, self.well, \n",
    "                                       \"DEAD\", unit_type, 0, len(self.non_dud_alignments), \"DEAD\"]],\n",
    "                                     columns = columns)\n",
    "            return output_df, output_df\n",
    "        \n",
    "        # If there are no variable positions, return wild type with the average\n",
    "        # number of counts\n",
    "        if len(all_variable_positions) == 0:\n",
    "            \n",
    "            # Get the mean read depth over all positions.\n",
    "            average_counts_by_position = int(np.mean(total_count_array))\n",
    "            \n",
    "            # Create an output dataframe and return\n",
    "            output_df = pd.DataFrame([[self.index_plate, self.plate_nickname, self.well, \n",
    "                                       \"WT\", unit_type, 1 - variable_thresh,\n",
    "                                       average_counts_by_position, \"WT\"]],\n",
    "                                     columns = columns)\n",
    "            return output_df, output_df\n",
    "                \n",
    "        # Get the variable frequencies\n",
    "        variable_freqs = np.transpose(unit_freq_array[:, all_variable_positions])\n",
    "        total_counts = total_count_array[all_variable_positions]\n",
    "\n",
    "        # Identify non-zero positions\n",
    "        nonzero_inds = np.argwhere(variable_freqs != 0)\n",
    "    \n",
    "        # Pull the variable amino acid positons, their frequencies/counts, and \n",
    "        # the associated amino acids. Also update positions for output: the offset\n",
    "        # is added to match the desired indexing of the user\n",
    "        variable_positions = (all_variable_positions[nonzero_inds[:, 0]]) + pos_offset\n",
    "        variable_expectation = expectation_array[nonzero_inds[:, 0]]\n",
    "        variable_total_counts = total_counts[nonzero_inds[:, 0]]\n",
    "        variable_units = unit_array[nonzero_inds[:, 1]]\n",
    "        nonzero_freqs = variable_freqs[nonzero_inds[:, 0], nonzero_inds[:, 1]]\n",
    "        \n",
    "        # We cannot have more counts than seqpairs\n",
    "        assert variable_total_counts.max() <= len(self.non_dud_alignments), \"Counting error\"\n",
    "        \n",
    "        # Format for output and convert to a dataframe\n",
    "        output_formatted = [[self.index_plate, self.plate_nickname, self.well, \n",
    "                           position, unit, freq, depth, flag] for \n",
    "                           position, unit, freq, depth, flag in \n",
    "                           zip(variable_positions, variable_units, nonzero_freqs,\n",
    "                              variable_total_counts, variable_expectation)]\n",
    "        output_df = pd.DataFrame(output_formatted, columns = columns)\n",
    "        \n",
    "        # Get the max output\n",
    "        freq_and_pos = output_df.loc[:, [unit_pos, \"AlignmentFrequency\"]]\n",
    "        max_inds = freq_and_pos.groupby(unit_pos).idxmax().AlignmentFrequency.values\n",
    "        max_by_position = output_df.loc[max_inds]\n",
    "        \n",
    "        return output_df, max_by_position\n",
    "    \n",
    "    # Write a function that generates the unpaired analysis outputs for \n",
    "    # both basepairs and amino acids\n",
    "    def analyze_unpaired_counts(self, variable_thresh):\n",
    "        \n",
    "        # Get the output format for basepairs\n",
    "        (self._unpaired_bp_output,\n",
    "         self._unpaired_bp_output_max) = self.analyze_unpaired_counts_generic(self.unit_bp_freqs_no_gaps,\n",
    "                                                                              self.bp_position_counts,\n",
    "                                                                              self.all_variable_bp_positions,\n",
    "                                                                              self.variable_bp_type,\n",
    "                                                                              BP_ARRAY, \"Bp\", variable_thresh,\n",
    "                                                                              self.bp_ind_start)\n",
    "        \n",
    "        # Get the output format for amino acids\n",
    "        (self._unpaired_aa_output,\n",
    "         self._unpaired_aa_output_max) = self.analyze_unpaired_counts_generic(self.unit_aa_freqs_no_gaps,\n",
    "                                                                              self.aa_position_counts,\n",
    "                                                                              self.all_variable_aa_positions,\n",
    "                                                                              self.variable_aa_type,\n",
    "                                                                              AA_ARRAY, \"Aa\", variable_thresh,\n",
    "                                                                              self.aa_ind_start)\n",
    "    \n",
    "    \n",
    "    # Write a function that analyzes and reports paired counts\n",
    "    def analyze_paired_counts_generic(self, variable_positions, all_counts, unit_array,\n",
    "                                      reference_sequence, variable_thresh, variable_count,\n",
    "                                      pos_offset):\n",
    "        \n",
    "        # Define output columns\n",
    "        columns = (\"IndexPlate\", \"Plate\", \"Well\", \"VariantCombo\", \"VariantsFound\",\n",
    "                   \"AlignmentFrequency\", \"WellSeqDepth\", \"VariantSequence\")\n",
    "        \n",
    "        # If there are no usable reads, return a dead dataframe\n",
    "        if not self.usable_reads:\n",
    "            return pd.DataFrame([[self.index_plate, self.plate_nickname, self.well,\n",
    "                                  \"DEAD\", 0, 0, 0, \"DEAD\"]], columns = columns)\n",
    "        \n",
    "        # Get the number of positions\n",
    "        n_positions = len(variable_positions)            \n",
    "\n",
    "        # Get the counts of alignments that are paired end\n",
    "        paired_alignment_inds = np.array([i for i, seqpair in enumerate(self.non_dud_alignments)\n",
    "                                          if seqpair.is_paired_post_alignment_qc()])\n",
    "        \n",
    "        # If there are no paired reads, return a dead dataframe\n",
    "        n_paired = len(paired_alignment_inds)\n",
    "        if n_paired < variable_count:\n",
    "            \n",
    "            # Create a dataframe and return\n",
    "            return pd.DataFrame([[self.index_plate, self.plate_nickname, self.well,\n",
    "                                  \"DEAD\", n_paired, 0, average_counts_by_position, \"DEAD\"]],\n",
    "                                columns = columns)\n",
    "        \n",
    "        # Get the counts for the paired alignment seqpairs\n",
    "        paired_alignment_counts = all_counts[paired_alignment_inds]\n",
    "        \n",
    "        # Get the mean read depth over all positions.\n",
    "        average_counts_by_position = int(np.mean(paired_alignment_counts.sum(axis = (0, 1))))\n",
    "        \n",
    "        # If there are no variable positions, return wild type with the average number of counts\n",
    "        if n_positions == 0:\n",
    "            \n",
    "            # Create a dataframe and return\n",
    "            return pd.DataFrame([[self.index_plate, self.plate_nickname, self.well,\n",
    "                                  \"WT\", 0, 1 - variable_thresh,\n",
    "                                  average_counts_by_position, reference_sequence]],\n",
    "                                columns = columns)\n",
    "\n",
    "        # Get the positions with variety\n",
    "        variable_position_counts = paired_alignment_counts[:, :, variable_positions]\n",
    "\n",
    "        # Make sure all passed QC. This means that the sum over the last two indices\n",
    "        # is equal to the number of amino acids. This works because amino acids are only\n",
    "        # counted if they pass QC: for all to pass QC they must all have an index at some\n",
    "        # position\n",
    "        passing_qc = variable_position_counts[variable_position_counts.sum(axis = (1, 2)) == n_positions]\n",
    "        \n",
    "        # If too few pass QC, return a dead dataframe\n",
    "        n_passing = len(passing_qc)\n",
    "        if  n_passing < variable_count:\n",
    "            \n",
    "            # Create a dataframe and return\n",
    "            return pd.DataFrame([[self.index_plate, self.plate_nickname, self.well,\n",
    "                                  \"DEAD\", n_paired, 0, average_counts_by_position, \"DEAD\"]],\n",
    "                                columns = columns)\n",
    "\n",
    "        # Get the unique sequences that all passed QC\n",
    "        unique_binary_combos, unique_counts = np.unique(passing_qc, axis = 0, return_counts = True)\n",
    "\n",
    "        # We cannot have more counts than paired seqpairs\n",
    "        assert unique_counts.max() <= len(paired_alignment_inds), \"Counting error\"\n",
    "        \n",
    "        # Get a frequency array\n",
    "        unique_freqs = unique_counts / unique_counts.sum()\n",
    "\n",
    "        # Loop over the unique combos and format for output\n",
    "        output = [None] * len(unique_counts)\n",
    "        for unique_counter, unique_binary_combo in enumerate(unique_binary_combos):\n",
    "\n",
    "            # Get the index profile. This maps each position to a unit position\n",
    "            # in either `BP_ARRAY` or `AA_ARRAY`\n",
    "            index_profile = np.argwhere(np.transpose(unique_binary_combo == 1))\n",
    "\n",
    "            # Get the position and amino acid.\n",
    "            unique_position_array = variable_positions[index_profile[:, 0]]\n",
    "            unique_combo = unit_array[index_profile[:, 1]]\n",
    "\n",
    "            # Make sure the output is sorted\n",
    "            assert np.all(np.diff(unique_position_array)), \"Output not sorted\"\n",
    "\n",
    "            # Construct a sequence based on the reference\n",
    "            # Construct a combo name based on the combo and position\n",
    "            new_seq = list(reference_sequence)\n",
    "            combo_name = [None] * n_positions\n",
    "            for combo_ind, (pos, unit) in enumerate(zip(unique_position_array, unique_combo)):\n",
    "\n",
    "                # Update the sequence\n",
    "                new_seq[pos] = unit\n",
    "\n",
    "                # Update the combo name. Add the offset to the position index to get\n",
    "                # the start id of the reference seqeunce\n",
    "                combo_name[combo_ind] = f\"{reference_sequence[pos]}{pos + pos_offset}{unit}\"\n",
    "\n",
    "            # Convert the new seq and new combo into strings\n",
    "            new_seq = \"\".join(new_seq)\n",
    "            combo_name = \"_\".join(combo_name)\n",
    "\n",
    "            # Record output\n",
    "            output[unique_counter] = [self.index_plate, self.plate_nickname, self.well,\n",
    "                                     combo_name, n_positions, unique_freqs[unique_counter],\n",
    "                                      unique_counts[unique_counter], new_seq]\n",
    "\n",
    "        # Convert output to a dataframe\n",
    "        return pd.DataFrame(output, columns = columns)\n",
    "                                      \n",
    "    # Analyze the paired data for both amino acids and basepairs                              \n",
    "    def analyze_paired_counts(self, variable_thresh, variable_count):\n",
    "        \n",
    "        # Analyze the paired data for both amino acids and basepairs\n",
    "        self._paired_bp_output = self.analyze_paired_counts_generic(self.all_variable_bp_positions,\n",
    "                                                                    self.all_bp_counts, BP_ARRAY,\n",
    "                                                                    self.reference_sequence,\n",
    "                                                                    variable_thresh, variable_count,\n",
    "                                                                    self.bp_ind_start)\n",
    "        \n",
    "        self._paired_aa_output = self.analyze_paired_counts_generic(self.all_variable_aa_positions,\n",
    "                                                                    self.all_aa_counts, AA_ARRAY,\n",
    "                                                                    self.reference_sequence_aa,\n",
    "                                                                    variable_thresh, variable_count,\n",
    "                                                                    self.aa_ind_start)\n",
    "        \n",
    "    # Write a function that outputs adapterless fastq files for all paired end seqpairs\n",
    "    # Note that the reverse complement of \n",
    "    def write_fastqs(self):\n",
    "        \n",
    "        # Identify the paired end sequence pairs\n",
    "        paired_end_alignments = tuple(filter(lambda x: x.is_paired(), self.all_seqpairs))\n",
    "        \n",
    "        # Build a list of sequences to save\n",
    "        f_records_to_save = [seqpair.f_adapterless for seqpair in paired_end_alignments]\n",
    "        r_records_to_save = [seqpair.sliced_r for seqpair in paired_end_alignments]\n",
    "        assert len(f_records_to_save) == len(r_records_to_save), \"Mismatch in number of paired ends\"\n",
    "            \n",
    "        # Save the records\n",
    "        with open(os.path.join(self.fasta_loc, \"F\", f\"{self.index_plate}-{self.well}_R1.fastq\"), \"w\") as f:\n",
    "            SeqIO.write(f_records_to_save, f, \"fastq\")\n",
    "        with open(os.path.join(self.fasta_loc, \"R\", f\"{self.index_plate}-{self.well}_R2.fastq\"), \"w\") as f:\n",
    "            SeqIO.write(r_records_to_save, f, \"fastq\")\n",
    "            \n",
    "    # Write a function that returns all pairwise alignments formatted for saving\n",
    "    def format_alignments(self):\n",
    "        \n",
    "        # Write a function that formats all alignments in a well\n",
    "        formatted_alignments = [\"\"] * int(len(self.all_seqpairs) * 3)\n",
    "        alignment_counter = 0\n",
    "        for pair_ind, seqpair in enumerate(self.all_seqpairs):\n",
    "\n",
    "            # Add a header row\n",
    "            formatted_alignments[alignment_counter] = f\"\\nAlignment {pair_ind}:\"\n",
    "            alignment_counter += 1\n",
    "\n",
    "            # If we are using the forward alignment, add to the list\n",
    "            if seqpair.use_f_alignment:\n",
    "                formatted_alignments[alignment_counter] = pairwise2.format_alignment(*seqpair.f_alignment)\n",
    "                alignment_counter += 1\n",
    "\n",
    "            # If we are using the reverse alignment, add to the list\n",
    "            if seqpair.use_r_alignment:\n",
    "                formatted_alignments[alignment_counter] = pairwise2.format_alignment(*seqpair.r_alignment)\n",
    "                alignment_counter += 1\n",
    "\n",
    "        # Join as one string and return with plate and well information\n",
    "        return (self.alignment_loc, \"\\n\".join(formatted_alignments))\n",
    "        \n",
    "        \n",
    "    # Define properties\n",
    "    @property\n",
    "    def all_seqpairs(self):\n",
    "        return self._all_seqpairs\n",
    "        \n",
    "    @property\n",
    "    def expected_variable_bp_positions(self):\n",
    "        return self._expected_variable_bp_positions\n",
    "    \n",
    "    @property\n",
    "    def expected_variable_aa_positions(self):\n",
    "        return self._expected_variable_aa_positions\n",
    "    \n",
    "    @property\n",
    "    def index_plate(self):\n",
    "        return self._index_plate\n",
    "    \n",
    "    @property\n",
    "    def plate_nickname(self):\n",
    "        return self._plate_nickname\n",
    "    \n",
    "    @property\n",
    "    def well(self):\n",
    "        return self._well\n",
    "    \n",
    "    @property\n",
    "    def reference_sequence(self):\n",
    "        return self._reference_sequence\n",
    "    \n",
    "    @property\n",
    "    def reference_sequence_aa(self):\n",
    "        return self._reference_sequence_aa\n",
    "    \n",
    "    @property\n",
    "    def ref_len(self):\n",
    "        return self._ref_len\n",
    "    \n",
    "    @property\n",
    "    def n_aas(self):\n",
    "        return self._n_aas\n",
    "    \n",
    "    @property\n",
    "    def in_frame_ind(self):\n",
    "        return self._in_frame_ind\n",
    "    \n",
    "    @property\n",
    "    def bp_ind_start(self):\n",
    "        return self._bp_ind_start\n",
    "    \n",
    "    @property\n",
    "    def aa_ind_start(self):\n",
    "        return self._aa_ind_start\n",
    "    \n",
    "    @property\n",
    "    def fasta_loc(self):\n",
    "        return self._fasta_loc\n",
    "    \n",
    "    @property\n",
    "    def alignment_loc(self):\n",
    "        return self._alignment_loc\n",
    "    \n",
    "    @property\n",
    "    def expected_bps(self):\n",
    "        return self._expected_bps\n",
    "    \n",
    "    @property\n",
    "    def expected_aas(self):\n",
    "        return self._expected_aas\n",
    "        \n",
    "    @property\n",
    "    def non_dud_alignments(self):\n",
    "        return self._non_dud_alignments\n",
    "    \n",
    "    @property\n",
    "    def usable_reads(self):\n",
    "        return self._usable_reads\n",
    "    \n",
    "    @property\n",
    "    def all_bp_counts(self):\n",
    "        return self._all_bp_counts\n",
    "    \n",
    "    @property\n",
    "    def all_aa_counts(self):\n",
    "        return self._all_aa_counts\n",
    "    \n",
    "    @property\n",
    "    def unit_bp_counts_no_gaps(self):\n",
    "        return self._unit_bp_counts_no_gaps\n",
    "    \n",
    "    @property\n",
    "    def unit_bp_freqs_no_gaps(self):\n",
    "        return self._unit_bp_freqs_no_gaps\n",
    "    \n",
    "    @property\n",
    "    def unit_aa_counts_no_gaps(self):\n",
    "        return self._unit_aa_counts_no_gaps\n",
    "    \n",
    "    @property\n",
    "    def unit_aa_freqs_no_gaps(self):\n",
    "        return self._unit_aa_freqs_no_gaps\n",
    "    \n",
    "    @property\n",
    "    def unit_bp_counts(self):\n",
    "        return self._unit_bp_counts\n",
    "    \n",
    "    @property\n",
    "    def unit_bp_freqs(self):\n",
    "        return self._unit_bp_freqs\n",
    "    \n",
    "    @property\n",
    "    def bp_position_counts(self):\n",
    "        return self._bp_position_counts\n",
    "    \n",
    "    @property\n",
    "    def unit_aa_counts(self):\n",
    "        return self._unit_aa_counts\n",
    "    \n",
    "    @property\n",
    "    def unit_aa_freqs(self):\n",
    "        return self._unit_aa_freqs\n",
    "    \n",
    "    @property\n",
    "    def aa_position_counts(self):\n",
    "        return self._aa_position_counts\n",
    "    \n",
    "    @property\n",
    "    def all_variable_bp_positions(self):\n",
    "        return self._all_variable_bp_positions\n",
    "    \n",
    "    @property\n",
    "    def all_variable_aa_positions(self):\n",
    "        return self._all_variable_aa_positions\n",
    "    \n",
    "    @property\n",
    "    def variable_bp_type(self):\n",
    "        return self._variable_bp_type\n",
    "    \n",
    "    @property\n",
    "    def variable_aa_type(self):\n",
    "        return self._variable_aa_type\n",
    "    \n",
    "    @property\n",
    "    def unpaired_bp_output(self):\n",
    "        return self._unpaired_bp_output\n",
    "    \n",
    "    @property\n",
    "    def unpaired_bp_output_max(self):\n",
    "        return self._unpaired_bp_output_max\n",
    "    \n",
    "    @property\n",
    "    def unpaired_aa_output(self):\n",
    "        return self._unpaired_aa_output\n",
    "    \n",
    "    @property\n",
    "    def unpaired_aa_output_max(self):\n",
    "        return self._unpaired_aa_output_max\n",
    "    \n",
    "    @property\n",
    "    def paired_bp_output(self):\n",
    "        return self._paired_bp_output\n",
    "    \n",
    "    @property\n",
    "    def paired_aa_output(self):\n",
    "        return self._paired_aa_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write procedural functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate reference sequences with barcodes\n",
    "def load_refseq(ref_seq_loc):\n",
    "    \n",
    "    # Load the index map and reference sequence\n",
    "    index_map = pd.read_csv(\"/home/brucejwittmann/GitRepos/ssSeq/ssSeqSupport/IndexMap.csv\")\n",
    "    ref_seq_crude = pd.read_csv(\"/home/brucejwittmann/GitRepos/ssSeq/AlignmentDev/TestData/20200205_ssSeq/RefSeqs.csv\")\n",
    "\n",
    "    # Expand each reference sequence\n",
    "    updated_ref_array = []\n",
    "    for row in ref_seq_crude.itertuples(index = False):\n",
    "        updated_ref_array.extend([[row.PlateName, row.IndexPlate, well, row.ReferenceSequence, row.InFrameBase]\n",
    "                                 for well in ALLOWED_WELLS])\n",
    "\n",
    "    # Define the complete reference sequence dataframe\n",
    "    complete_ref_seq = pd.DataFrame(updated_ref_array, columns = (\"PlateName\", \"IndexPlate\", \"Well\", \"ReferenceSequence\", \"InFrameBase\"))\n",
    "\n",
    "    # Join on plate and well\n",
    "    merged_dfs = complete_ref_seq.merge(index_map, on = (\"IndexPlate\", \"Well\"))\n",
    "\n",
    "    # Map barcode to reference sequence, plate, and well\n",
    "    warnings.warn(\"Did not yet implement calculation of variable bases\")\n",
    "    bc_to_ref_plate_well = {(row.FBC, row.RBC): {\"IndexPlate\": row.IndexPlate,\n",
    "                                                 \"PlateNickname\": row.PlateName,\n",
    "                                                 \"Well\": row.Well,\n",
    "                                                 \"ReferenceSequence\": row.ReferenceSequence,\n",
    "                                                \"InFrameBase\": row.InFrameBase,\n",
    "                                                \"ExpectedVariablePositions\": np.array([], dtype = int),\n",
    "                                                \"BpIndStart\": 1,\n",
    "                                                \"AAIndStart\": 1}\n",
    "                           for row in merged_dfs.itertuples(index = False)}\n",
    "    \n",
    "    return bc_to_ref_plate_well\n",
    "\n",
    "# Write a function for loading and pairing fastq files\n",
    "def load_fastq(f_loc, r_loc):\n",
    "\n",
    "    # Create a dictionary that links id to sequence object\n",
    "    id_to_reads = {}\n",
    "    print(\"Loading forward reads...\")\n",
    "    all_f_recs = list(SeqIO.parse(f_loc, \"fastq\"))\n",
    "    for f_record in all_f_recs:\n",
    "        temp_record = SeqPair()\n",
    "        temp_record.assign_f(f_record)\n",
    "        id_to_reads[f_record.id] = temp_record\n",
    "    \n",
    "    # Associate reverse reads with the forward\n",
    "    print(\"Loading reverse reads...\")\n",
    "    all_r_recs = list(SeqIO.parse(r_loc, \"fastq\"))\n",
    "    for r_record in all_r_recs:\n",
    "\n",
    "        # If there is no partern in id_to_reads, create a new object \n",
    "        # and continue\n",
    "        if r_record.id not in id_to_reads:\n",
    "            temp_record = SeqPair()\n",
    "            temp_record.assign_r(r_record)\n",
    "            id_to_reads[r_record.id] = temp_record\n",
    "\n",
    "        # Otherwise, attach the reverse record\n",
    "        else:\n",
    "            id_to_reads[r_record.id].assign_r(r_record)\n",
    "            \n",
    "    # Only keep records that have a partner\n",
    "    return tuple(id_to_reads.values())\n",
    "\n",
    "# Write a function for filtering out bad seqpairs\n",
    "def qc_seqpairs(all_seqpairs, read_length = None, length_cutoff = 0.9, \n",
    "                average_q_cutoff = 25):\n",
    "    \n",
    "    print(\"Running read qc...\")\n",
    "    \n",
    "    # If we don't have the read length determine it\n",
    "    if read_length is None:\n",
    "\n",
    "        # Get the most common read length. We will assign this as our read length\n",
    "        all_readlengths = np.array([seqpair.read_lengths() for seqpair in all_seqpairs])\n",
    "        read_length = ss.mode(all_readlengths, axis = None, nan_policy = \"omit\").mode[0]\n",
    "        \n",
    "    # Calculate the read filter\n",
    "    read_filter = read_length * length_cutoff\n",
    "        \n",
    "    # Run QC on every read\n",
    "    for seqpair in all_seqpairs:\n",
    "        seqpair.qc_reads(read_filter, average_q_cutoff)\n",
    "    \n",
    "    # Eliminate any duds, which are those seqpairs with both a forward and a reverse that failed qc\n",
    "    no_duds = tuple(filter(lambda x: not x.is_dud(), all_seqpairs))\n",
    "    \n",
    "    return no_duds\n",
    "\n",
    "# Write a function for assigning seqpairs to a well\n",
    "def assign_seqpairs_to_well(filtered_seqpairs, bc_to_ref_plate_well, savedir):\n",
    "\n",
    "    # Loop over all seqpairs and assign to wells\n",
    "    print(\"Assigning sequences to wells...\")\n",
    "    well_pairs = {}\n",
    "    for pair in filtered_seqpairs:\n",
    "\n",
    "        # Grab the well ID and see if it is a real well. Continue\n",
    "        # if it is not. \"Fake\" wells are those that result from \n",
    "        # sequencing errors\n",
    "        well_id = (pair.f_barcode, pair.r_barcode)\n",
    "        if well_id not in bc_to_ref_plate_well:\n",
    "            continue\n",
    "        \n",
    "        # Check to see if we have seen this well already.\n",
    "        # If we have seen it, append to growing list. If we have not,\n",
    "        # start a new list\n",
    "        if well_id in well_pairs:\n",
    "            well_pairs[well_id].append(pair)\n",
    "        else:\n",
    "            well_pairs[well_id] = [pair]\n",
    "            \n",
    "    # Now build and return the well objects\n",
    "    return [Well(pair, bc_to_ref_plate_well[well_id], savedir) \n",
    "            for well_id, pair in well_pairs.items()] \n",
    "\n",
    "# Write a function that can process a single well\n",
    "def process_well(well, return_alignments = False, \n",
    "                 bp_q_cutoff = 30,\n",
    "                 variable_thresh = 0.1,\n",
    "                 variable_count = 1):\n",
    "\n",
    "    # Align\n",
    "    well.align()\n",
    "\n",
    "    # Analyze alignments. \n",
    "    has_reads = well.analyze_alignments(bp_q_cutoff, variable_count)\n",
    "\n",
    "    # If we don't have any reads that passed\n",
    "    # QC, skip straight to analyzing counts. \n",
    "    if has_reads:\n",
    "        # Build count matrices\n",
    "        well.build_unit_count_matrices()\n",
    "\n",
    "        # Identify variable positions\n",
    "        well.identify_variable_positions(variable_thresh)\n",
    "\n",
    "    # Analyze reads with decoupled counts\n",
    "    well.analyze_unpaired_counts(variable_thresh)\n",
    "\n",
    "    # Analyze reads with coupled counts\n",
    "    well.analyze_paired_counts(variable_thresh, variable_count)\n",
    "\n",
    "    # If we are returning alignments, generate them\n",
    "    if return_alignments:\n",
    "        formatted_alignments = well.format_alignments()\n",
    "    else:\n",
    "        formatted_alignments = None\n",
    "\n",
    "    # Return relevant information for downstream processing\n",
    "    return (well.unpaired_bp_output, well.unpaired_bp_output_max,\n",
    "            well.unpaired_aa_output, well.unpaired_aa_output_max,\n",
    "            well.paired_bp_output, well.paired_aa_output, \n",
    "            formatted_alignments) \n",
    "    \n",
    "def format_and_save_outputs(well_results, saveloc, return_alignments):\n",
    "\n",
    "    # Write a function that processes the output of analyzing all wells\n",
    "    unpacked_output = tuple(zip(*well_results))\n",
    "\n",
    "    # Concatenate all dataframes\n",
    "    full_dfs = tuple(pd.concat(df_list, ignore_index = True) for df_list in unpacked_output[:-1])\n",
    "\n",
    "    # Get just the max of each of the paired outputs\n",
    "    max_outs = [None, None]\n",
    "    for i, paired_df in enumerate(full_dfs[4:]):\n",
    "\n",
    "        # Get the columns of interest\n",
    "        limited_df = paired_df.loc[:, [\"IndexPlate\", \"Well\", \"AlignmentFrequency\"]]\n",
    "\n",
    "        # Group by plate and well\n",
    "        grouped_df = limited_df.groupby(by = [\"IndexPlate\", \"Well\"])\n",
    "        max_outs[i] = paired_df.loc[grouped_df.idxmax().AlignmentFrequency.values].copy()\n",
    "\n",
    "    # Loop over all dataframes, sort by plate and well, and save\n",
    "    savenames = (\"Bases_Decoupled_All.csv\", \"Bases_Decoupled_Max.csv\",\n",
    "                \"AminoAcids_Decoupled_All.csv\", \"AminoAcids_Decoupled_Max.csv\",\n",
    "                 \"Bases_Coupled_All.csv\", \"Combos_Coupled_All.csv\",\n",
    "                 \"Bases_Coupled_Max.csv\", \"Combos_Coupled_Max.csv\")\n",
    "    for savename, output_df in zip(savenames, chain(full_dfs, max_outs)):\n",
    "\n",
    "        # Sort by plate and well\n",
    "        output_df.sort_values(by = [\"IndexPlate\", \"Well\"], inplace = True)\n",
    "\n",
    "        # Save the dataframe\n",
    "        output_df.to_csv(os.path.join(saveloc, \"OutputCounts\", savename), index = False)\n",
    "\n",
    "    # Loop over and save all alignments if asked to do so\n",
    "    if return_alignments:\n",
    "        for savename, savestr in unpacked_output[-1]:\n",
    "            with open(savename, \"w\") as f:\n",
    "                f.write(savestr)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that runs deSeq\n",
    "def run_deseq(ref_seq_loc, forward_read_loc, reverse_read_loc,\n",
    "              saveloc, n_cpus, stop_after_qualities = False,\n",
    "              stop_after_fastq = False, return_alignments = False,\n",
    "              read_length = None, length_cutoff = 0.9,\n",
    "              average_q_cutoff = 25, bp_q_cutoff = 30,\n",
    "              variable_thresh = 0.1, variable_count = 1):\n",
    "    \n",
    "    # Load the reference sequence file and associate reference sequence\n",
    "    # information with wells\n",
    "    bc_to_ref_plate_well = load_refseq(ref_seq_loc)\n",
    "    \n",
    "    # Load fastq files\n",
    "    all_seqpairs = load_fastq(forward_read_loc, reverse_read_loc)\n",
    "    \n",
    "    # Filter the seqpairs\n",
    "    filtered_seqpairs = qc_seqpairs(all_seqpairs, \n",
    "                                    read_length = read_length,\n",
    "                                    length_cutoff = length_cutoff, \n",
    "                                    average_q_cutoff = average_q_cutoff)\n",
    "    \n",
    "    # Plot qualities\n",
    "    warnings.warn(\"Need to implement quality plotting\")\n",
    "#     plot_qualities()\n",
    "    \n",
    "    # Return if we stop after plot qualities\n",
    "    if stop_after_qualities:\n",
    "        return\n",
    "    \n",
    "    # Assign seqpairs to a well\n",
    "    all_wells = assign_seqpairs_to_well(filtered_seqpairs, bc_to_ref_plate_well, saveloc)\n",
    "    \n",
    "    # Save the fastq files\n",
    "    for well in all_wells:\n",
    "        well.write_fastqs()\n",
    "\n",
    "    # Return if we stop after fastq\n",
    "    if stop_after_fastq:\n",
    "        return\n",
    "        \n",
    "    # Complete the multiprocessing function, then process all wells\n",
    "    complete_multiprocessor = partial(process_well, \n",
    "                                      bp_q_cutoff = bp_q_cutoff,\n",
    "                                      return_alignments = return_alignments,\n",
    "                                     variable_thresh = variable_thresh,\n",
    "                                     variable_count = variable_count)\n",
    "        \n",
    "    # Multiprocess to handle wells\n",
    "    with Pool(n_cpus) as p:\n",
    "        processed_well_results = list(tqdm(p.imap_unordered(complete_multiprocessor, all_wells),\n",
    "                                      desc = \"Processing wells:\", total = len(all_wells)))\n",
    "        \n",
    "    # Handle processed output\n",
    "    format_and_save_outputs(processed_well_results, saveloc, return_alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brucejwittmann/anaconda3/envs/mlde_analyze/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: Did not yet implement calculation of variable bases\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading forward reads...\n",
      "Loading reverse reads...\n",
      "Running read qc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brucejwittmann/anaconda3/envs/mlde_analyze/lib/python3.7/site-packages/ipykernel_launcher.py:23: UserWarning: Need to implement quality plotting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning sequences to wells...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing wells:: 100%|| 477/477 [00:44<00:00, 10.60it/s]\n"
     ]
    }
   ],
   "source": [
    "run_deseq(\"./TestData/20200205_ssSeq/RefSeqs.csv\",\n",
    "          \"./TestData/20200205_ssSeq/CHL2_S199_L001_R1_001.fastq\",\n",
    "          \"./TestData/20200205_ssSeq/CHL2_S199_L001_R2_001.fastq\",\n",
    "          \"./\", 23)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
